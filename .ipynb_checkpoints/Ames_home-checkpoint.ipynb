{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Kaggle: Ames home sale\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "\n",
    "pd.set_option('display.mpl_style', 'default')\n",
    "pd.set_option('display.width', 3000)\n",
    "\n",
    "na_values=['NA', 'N/A']\n",
    "data_train = pd.read_csv('train.csv', na_values=na_values)\n",
    "data_train = data_train.drop(data_train[(data_train.LotArea > 50000) & (data_train.SalePrice < 200000)].index)\n",
    "data_test = pd.read_csv('test.csv', na_values=na_values)\n",
    "data_all = pd.concat([data_train, data_test])\n",
    "data_all = data_all.drop(['SalePrice'], axis=1)\n",
    "#data_all.to_csv('data_all.csv')\n",
    "m_train = data_train.shape[0]\n",
    "m_test = data_test.shape[0]\n",
    "#data_all.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data_all['Street'].value_counts().sort_index().plot(kind='bar')\n",
    "\n",
    "# plt.figure(figsize=(18,6))\n",
    "#data_train['TotalBsmtSF'].value_counts().sort_index().plot(kind='bar')\n",
    "#data_train[data_train['LotArea']<20000].sample(1000).plot.scatter(x='YearBuilt', y='SalePrice')\n",
    "#data_train[data_train['LotArea']<15000].sample(200).plot.scatter(x='TotRmsAbvGrd', y='SalePrice')\n",
    "#sns.kdeplot(data_train[data_train['SalePrice']<500000]['SalePrice'])\n",
    "#sns.lmplot(x='LotArea', y='SalePrice', data=data_train[data_train['LotArea']<100000], fit_reg=False)\n",
    "#sns.jointplot(x='TotalBsmtSF', y='SalePrice', data=data_train[data_train['LotArea']<20000].sample(len(data_train)/6))\n",
    "#sns.heatmap(data_train.loc[:, ['LotFrontage', 'LotArea', 'SalePrice']].corr(), annot=True)\n",
    "# data_train[data_train['LotArea']<1]['LotArea'].plot.hist(bins=30)\n",
    "#data_train[data_train['TotalBsmtSF']<100000]['TotalBsmtSF'].plot.hist(bins=50)\n",
    "# plt.figure(figsize=(18,18))\n",
    "# sns.heatmap(data_train.iloc[:,1:].corr(), vmax=1, square=True)\n",
    "#np.log(data_train['SalePrice']).plot.hist(bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(12,12))\n",
    "# sns.set()\n",
    "# n=10\n",
    "# corrmat = data_train.iloc[:, 1:].corr()\n",
    "# cols = corrmat.nlargest(n, 'SalePrice')['SalePrice'].index\n",
    "# cc = np.corrcoef(data_train[cols].values.T)\n",
    "# sns.heatmap(cc, cbar=True, annot=False, square=True, yticklabels=cols.values, xticklabels=cols.values)\n",
    "\n",
    "# #pair plot\n",
    "# cols = ['SalePrice', 'OverallQual', 'GrLivArea', 'TotalBsmtSF', 'GarageCars', 'GarageArea', '1stFlrSF', 'FullBath', \n",
    "#         'TotRmsAbvGrd', 'YearBuilt']\n",
    "# sns.pairplot(data_train[cols], size=2.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Data pre-processing: imputer and dtype transform\n",
    "##imputing missing values: numbered values fill(0), categorical values fill('None'), features with single-digits NA fill with mode()[0]\n",
    "# #check missing data\n",
    "# data_all_na = data_all.isnull().sum()\n",
    "# data_all_na = data_all_na.drop(data_all_na[data_all_na==0].index).sort_values(ascending=False)\n",
    "# missing_data = pd.DataFrame({'missing': data_all_na})\n",
    "\n",
    "fill_none_features = ['PoolQC', 'Alley', 'Fence', 'MiscFeature', 'FireplaceQu', 'GarageType', 'GarageFinish', \n",
    "                      'GarageQual', 'GarageCond', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', \n",
    "                      'BsmtFinType2', 'MasVnrType',]\n",
    "fill_zero_features = ['GarageArea', 'GarageCars', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', '1stFlrSF', '2ndFlrSF',\n",
    "                      'TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath', 'MasVnrArea']\n",
    "fill_mode_features = ['Functional', 'MSZoning', 'Electrical', 'KitchenQual', 'Exterior1st', 'Exterior2nd', 'SaleType']\n",
    "\n",
    "def Imputing_na(data, fill_none_features, fill_zero_features, fill_mode_features):\n",
    "    for feature in fill_none_features:\n",
    "        data[feature] = data[feature].fillna('None')\n",
    "    for feature in fill_zero_features:\n",
    "        data[feature] = data[feature].fillna(0)\n",
    "    for feature in fill_mode_features:\n",
    "        data[feature] = data[feature].fillna(data[feature].mode()[0])\n",
    "    return data\n",
    "\n",
    "data_all = Imputing_na(data_all, fill_none_features, fill_zero_features, fill_mode_features)\n",
    "\n",
    "##dtype transform: numeric to categorical\n",
    "dtype_features = ['MSSubClass', 'MoSold', 'OverallCond']\n",
    "for feature in dtype_features:\n",
    "    data_all[feature] = data_all[feature].apply(str)\n",
    "    \n",
    "##LotFrontage imputering\n",
    "data_all['LotFrontage'] = data_all.groupby(\"Neighborhood\")['LotFrontage'].transform(lambda x: x.fillna(x.median()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Feature engineering\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.special import boxcox1p\n",
    "\n",
    "#Add features: 'TotalSF'\n",
    "data_all['TotalSF'] = data_all['TotalBsmtSF'] + data_all['1stFlrSF'] + data_all['2ndFlrSF']\n",
    "data_all['BuiltAge'] = np.abs(data_all['YrSold'] - data_all['YearBuilt'])\n",
    "data_all['RemodelAge'] = np.abs(data_all['YrSold'] - data_all['YearRemodAdd'])\n",
    "#Convert features\n",
    "# def BuiltAge(data):\n",
    "#     data['BuiltAge'] = data['YrSold'] - data['YearBuilt']\n",
    "#     bins = (-5,1,4,8,12,16,20,25,30,35,40,50,60,80,100,150)\n",
    "#     group_names=['1','4','8','12','16','20','25','30','35','40','50','60','80','100','150']\n",
    "#     categories = pd.cut(data['BuiltAge'], bins, labels=group_names)\n",
    "#     data['BuiltAge'] = categories\n",
    "#     return data\n",
    "\n",
    "# def RemodAge(data):\n",
    "#     data['RemodelAge'] = data['YrSold'] - data['YearRemodAdd']\n",
    "#     bins = (-5, 1, 4, 8, 12, 18, 25, 40, 60)\n",
    "#     group_names=['1', '4', '8', '12', '18', '25', '40', '60']\n",
    "#     categories = pd.cut(data['RemodelAge'], bins, labels=group_names)\n",
    "#     data['RemodelAge'] = categories\n",
    "#     return data\n",
    "\n",
    "#Feature transform\n",
    "def Stand_Scaler(data):\n",
    "    norm_features = ['PoolArea', 'BsmtUnfSF', 'GarageArea', 'TotalBsmtSF']\n",
    "    Scaler = StandardScaler()\n",
    "    Scaler = Scaler.fit(data[norm_features])\n",
    "    dt = Scaler.transform(data[norm_features])\n",
    "    dt = pd.DataFrame(dt)\n",
    "\n",
    "    for i, feature in enumerate(norm_features):\n",
    "        data.loc[:, feature] = dt.iloc[:, i]\n",
    "    return data\n",
    "\n",
    "def encode_features(data, features):\n",
    "    for feature in features:\n",
    "        le = preprocessing.LabelEncoder().fit(data[feature])\n",
    "        data[feature] = le.transform(data[feature])\n",
    "    return data\n",
    "\n",
    "def feature_transform(data):\n",
    "    e_features = ['Alley', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'BsmtQual', \n",
    "                  'Condition1', 'Condition2', 'Electrical', 'ExterCond', 'ExterQual', \n",
    "                  'Fence','FireplaceQu', 'Functional', 'GarageCars', 'GarageCond', 'GarageFinish', 'GarageQual', \n",
    "                  'Heating', 'HeatingQC', 'HouseStyle','KitchenQual', 'LandSlope', 'MasVnrType', 'MiscFeature', \n",
    "                  'MoSold', 'MSSubClass', 'PavedDrive', 'PoolQC', 'RoofMatl', 'Street']\n",
    "#     data = BuiltAge(data)\n",
    "#     data = RemodAge(data)\n",
    "    data = Stand_Scaler(data)\n",
    "    data = encode_features(data, e_features)\n",
    "    return data\n",
    "\n",
    "data_all=feature_transform(data_all)\n",
    "data_all = data_all.drop(['Utilities', 'GarageYrBlt', 'YrSold', 'YearBuilt', 'YearRemodAdd', 'MiscFeature', 'PoolQC',\n",
    "                          'Alley'], axis=1)\n",
    "\n",
    "#column re-arrangement\n",
    "cols = data_all.columns.tolist()\n",
    "cols = cols[41:42] + cols[48:49] + cols[:41] + cols[42:48] + cols[49:]\n",
    "data_all = data_all[cols]\n",
    "\n",
    "#Categorical features without ordering\n",
    "# categ_features = ['BldgType', 'CentralAir', 'Exterior1st', 'Exterior2nd', 'Foundation', 'GarageType', 'LandContour',\n",
    "#                   'LotConfig', 'LotShape', 'MSZoning', 'Neighborhood', 'RoofStyle', 'SaleCondition', 'SaleType']\n",
    "\n",
    "data_all = pd.get_dummies(data_all)\n",
    "\n",
    "data_all.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import skew\n",
    "from scipy.special import boxcox1p\n",
    "\n",
    "#Calculate skewness of numeric features\n",
    "float_numeric = ['BuiltAge', 'RemodelAge', 'GrLivArea', '1stFlrSF', '2ndFlrSF', 'MasVnrArea', 'OpenPorchSF', 'EnclosedPorch', \n",
    "                 'BsmtFinSF1', 'BsmtFinSF2', '3SsnPorch', 'MiscVal','WoodDeckSF', 'ScreenPorch', 'LowQualFinSF', \n",
    "                 'TotalSF', 'LotArea', 'LotFrontage']\n",
    "skewness = data_all[float_numeric].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\n",
    "skewness = pd.DataFrame({'skewness': skewness})\n",
    "skew_features = skewness[abs(skewness)>0.5].index\n",
    "#BoxCox transform of highly skewed features\n",
    "\n",
    "lam = 0.2\n",
    "for feature in skew_features:\n",
    "    data_all[feature] = boxcox1p(data_all[feature], lam)\n",
    "#data_all[skew_features] = np.log1p(data_all[skew_features])\n",
    "skewness_bc = data_all[skew_features].apply(lambda x: skew(x)).sort_values(ascending=False)\n",
    "#skewness, skewness_bc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# feature engineering: filling null data 'LotFrontage' by XGBoost\n",
    "\n",
    "# import xgboost as xgb\n",
    "# from xgboost import XGBRegressor\n",
    "\n",
    "# data_LF_notnull = data_all.loc[data_all.LotFrontage.notnull()]\n",
    "# data_LF_isnull = data_all.loc[data_all.LotFrontage.isnull()]\n",
    "\n",
    "# x_lf_train = data_LF_notnull.iloc[:, 2:]\n",
    "# y_lf_train = data_LF_notnull.iloc[:, 1]\n",
    "# x_lf_test = data_LF_isnull.iloc[:, 2:]\n",
    "\n",
    "# xgbr = xgb.XGBRegressor(n_estimator=100)\n",
    "# xgbr.fit(x_lf_train, y_lf_train)\n",
    "# y_lf_hat = xgbr.predict(x_lf_test)\n",
    "# data_all.loc[(data_all.LotFrontage.isnull()), 'LotFrontage'] = y_lf_hat\n",
    "              \n",
    "# data_all_lf = data_all.LotFrontage\n",
    "# data_all_lf = data_all_lf.reshape(-1,1)\n",
    "# data_all_lf = boxcox1p(data_all_lf, 0.2)\n",
    "# data_all['LotFrontage'] = data_all_lf\n",
    "\n",
    "# data_all.to_csv('data_all_eng.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "d_train = data_all.iloc[:m_train, :]\n",
    "d_test = data_all.iloc[m_train:, :]\n",
    "\n",
    "x = d_train.iloc[:, 1:]\n",
    "y = np.log(data_train['SalePrice'])\n",
    "\n",
    "test_size = 0.3\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = test_size, random_state = 1)\n",
    "mse_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Linear regression model\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LassoCV, RidgeCV, ElasticNetCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import PolynomialFeatures, RobustScaler\n",
    "\n",
    "degree = 2\n",
    "linear_models =[Pipeline([('Scale', RobustScaler()),\n",
    "                            ('Poly', PolynomialFeatures(degree=degree)),\n",
    "                              ('Linear', LassoCV(alphas = np.logspace(-3,2,5), cv=3, fit_intercept=False))]),\n",
    "                Pipeline([('Scale', RobustScaler()),\n",
    "                            ('Poly', PolynomialFeatures(degree=degree)),\n",
    "                              ('Linear', RidgeCV(alphas = np.logspace(-3,2,5), cv=3, fit_intercept=False))]),\n",
    "                Pipeline([('Scale', RobustScaler()),\n",
    "                            ('Poly', PolynomialFeatures(degree=degree, include_bias=True)),\n",
    "                          ('Linear', ElasticNetCV(l1_ratio=[0.1, 0.2, 0.3, 0.5, 0.9, 0.99, 1], alphas=np.logspace(-3,2,5), fit_intercept=False, max_iter=1e3, cv=3))])\n",
    "               ]\n",
    "\n",
    "mse_list = []\n",
    "for i in range(3):\n",
    "    model = linear_models[i]\n",
    "    model.fit(x_train, y_train)\n",
    "    linear = model.get_params('linear')['Linear']\n",
    "    if hasattr(linear, 'alpha_'):\n",
    "        print 'alpha for model %d: %.2f.' % (i+1, linear.alpha_)\n",
    "    if hasattr(linear, 'l1_ratio_'):\n",
    "        print 'l1_ratio for model %d: %.2f.' % (i+1, linear.l1_ratio_)\n",
    "    y_hat = model.predict(x_test)\n",
    "    mse = mean_squared_error(y_hat, y_test)\n",
    "    mse_list.append(mse)\n",
    "print (mse_list)\n",
    "\n",
    "lin = linear_models[mse_list.index(min(mse_list))]\n",
    "lin.fit(x_train, y_train)\n",
    "y_hat_lin = lin.predict(x_test)\n",
    "print lin\n",
    "print mean_squared_error(y_hat_lin, y_test)\n",
    "mse_dict['Linear'] = mean_squared_error(y_hat_lin, y_test)\n",
    "\n",
    "#eli5.show_weights(lin, feature_names = list(x_train.columns))\n",
    "\n",
    "# y_pred_lin = lin.predict(d_test.drop('Id', axis=1))\n",
    "# Ames_lin = pd.DataFrame(data={'Id': d_test['Id'], 'SalePrice': np.exp(y_pred_lin)})\n",
    "# Ames_lin.to_csv('Ames_lin.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Kernel Ridge Regression\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer, mean_squared_error\n",
    "\n",
    "krr = KernelRidge(alpha = 0.5, kernel='polynomial', degree=2, coef0=5)\n",
    "krr.fit(x_train, y_train)\n",
    "y_hat_krr = krr.predict(x_test)\n",
    "print krr\n",
    "print mean_squared_error(y_hat_krr, y_test)\n",
    "mse_dict['KRR'] = mean_squared_error(y_hat_krr, y_test)\n",
    "\n",
    "# y_pred_krr = krr.predict(d_test.drop('Id', axis=1))\n",
    "# Ames_krr = pd.DataFrame(data={'Id': d_test['Id'], 'SalePrice': np.exp(y_pred_krr)})\n",
    "# Ames_krr.to_csv('Ames_krr.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Bayesian\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Decison tree\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer, mean_squared_error\n",
    "import eli5\n",
    "\n",
    "dtr = DecisionTreeRegressor()\n",
    "parameters = {'criterion': ['mse'], 'max_depth': [5, 8, 10,15,20], 'max_features': ['log2', 'sqrt', 'auto'], 'min_samples_leaf': [2,3,5,8]}\n",
    "grid = GridSearchCV(dtr, parameters, scoring = make_scorer(mean_squared_error), n_jobs=-1, verbose=1)\n",
    "grid.fit(x_train, y_train)\n",
    "dtr = grid.best_estimator_\n",
    "y_hat_dtr = dtr.predict(x_test)\n",
    "print dtr\n",
    "print mean_squared_error(y_hat_dtr, y_test)\n",
    "mse_dict['DecisionTree'] = mean_squared_error(y_test, y_hat_dtr)\n",
    "\n",
    "# y_pred_dtr = dtr.predict(d_test.drop('Id', axis=1))\n",
    "# Ames_dtr = pd.DataFrame(data={'Id': d_test['Id'], 'SalePrice': np.exp(y_pred_dtr)})\n",
    "# Ames_dtr.to_csv('Ames_dtr.csv')\n",
    "eli5.show_weights(dtr, feature_names=list(x_train.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Ensemble: Random Forest\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import make_scorer, mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "rfr = RandomForestRegressor(n_jobs=-1)\n",
    "\n",
    "parameters = {'n_estimators':[20,100],\n",
    "             'max_features':['log2','sqrt','auto'],\n",
    "             'criterion':['mse'],\n",
    "              'max_depth':[3,5,8],\n",
    "             'min_samples_split':[2,3,5],\n",
    "             'min_samples_leaf':[3,5,8]}\n",
    "grid = GridSearchCV(rfr, parameters, scoring = make_scorer(mean_squared_error),n_jobs=-1,verbose=1)\n",
    "grid = grid.fit(x_train, y_train)\n",
    "rfr=grid.best_estimator_\n",
    "rfr.fit(x_train, y_train)\n",
    "\n",
    "#serializing model using either pickle or joblib\n",
    "# filename = rfr_final.sav\n",
    "# pickle.dump(rfr, open(filename, 'wb'))\n",
    "# loaded_model = pickle.load(open(filename, 'rb'))\n",
    "\n",
    "y_hat_rfr = rfr.predict(x_test)\n",
    "print rfr\n",
    "print (mean_squared_error(y_test, y_hat_rfr))\n",
    "mse_dict['RandomForest'] = mean_squared_error(y_test, y_hat_rfr)\n",
    "\n",
    "# y_pred_rfr = rfr.predict(d_test.drop('Id', axis=1))\n",
    "# Ames_rfr = pd.DataFrame(data={'Id': d_test['Id'], 'SalePrice': np.exp(y_pred_rfr)})\n",
    "# Ames_rfr.to_csv('Ames_rfr.csv')\n",
    "#eli5.show_weights(rfr, feature_names=list(x_train.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Gradient Boosting\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "gbr = GradientBoostingRegressor()\n",
    "params = {'loss': ['ls'], 'learning_rate': [0.05, 0.1, 0.2], 'n_estimators': [20, 100, 1000], 'max_depth': [3,5,8],\n",
    "         'min_samples_split': [5,8,10], 'max_features': ['auto', 'log2', 'sqrt']}\n",
    "gbr=GridSearchCV(gbr, params, cv=3, n_jobs=-1, verbose=1)\n",
    "gbr.fit(x_train, y_train)\n",
    "y_hat_gbr = gbr.predict(x_test)\n",
    "print gbr\n",
    "print mean_squared_error(y_hat_gbr, y_test)\n",
    "mse_dict['GradientBoosting'] = mean_squared_error(y_hat_gbr, y_test)\n",
    "\n",
    "# y_pred_gb = gbr.predict(d_test.drop('Id', axis=1))\n",
    "# Ames_gb = pd.DataFrame(data={'Id': d_test['Id'], 'SalePrice': np.exp(y_pred_gb)})\n",
    "# Ames_gb.to_csv('Ames_gb.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#XGBoost\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "import eli5\n",
    "\n",
    "xgbr = xgb.XGBRegressor()\n",
    "\n",
    "params = {'max_depth': [3,5,8], 'learning_rate': [0.05, 0.1], 'n_estimators': [100, 1000], 'objective': ['reg:linear'], 'gamma': [i/10.0 for i in range(1,2)], 'reg_alpha': [i/10.0 for i in range(0,2)], 'reg_lambda': [0.5, 1.0], 'min_child_weight': [2, 3, 5]}\n",
    "\n",
    "xgbr = GridSearchCV(xgbr, params, cv=3, n_jobs=-1, verbose=1)\n",
    "xgbr.fit(x_train, y_train)\n",
    "y_hat_xgb = xgbr.predict(x_test)\n",
    "\n",
    "print xgbr\n",
    "print mean_squared_error(y_hat_xgb, y_test)\n",
    "mse_dict['XGBoost'] = mean_squared_error(y_hat_xgb, y_test)\n",
    "\n",
    "# y_pred_xgb = xgbr.predict(d_test.drop('Id', axis=1))\n",
    "# Ames_xgb = pd.DataFrame(data={'Id': d_test['Id'], 'SalePrice': np.exp(y_pred_xgb)})\n",
    "# Ames_xgb.to_csv('Ames_xgb.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Light GBM\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "lgb = lgb.LGBMRegressor(boosting_type='gbdt', objective='regression', metric = 'mean_squared_error')\n",
    "params_grid = {'learning_rate': [0.02, 0.05, 0.1], 'n_estimator': [50, 100, 1000, 5000], 'max_depth': [5,10,20], 'num_leaves': [10, 20, 31, 40]}\n",
    "lgb = GridSearchCV(lgb, params_grid, n_jobs=-1, verbose=1)\n",
    "lgb.fit(x_train, y_train)\n",
    "\n",
    "y_hat_lgb = lgb.predict(x_test)\n",
    "print lgb\n",
    "print mean_squared_error(y_hat_lgb, y_test)\n",
    "mse_dict['LightGBM'] = mean_squared_error(y_hat_lgb, y_test)\n",
    "\n",
    "# y_pred_lgb = lgb.predict(d_test.drop('Id', axis=1))\n",
    "# Ames_lgb = pd.DataFrame(data={'Id': d_test['Id'], 'SalePrice': np.exp(y_pred_lgb)})\n",
    "# Ames_lgb.to_csv('Ames_lgb.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Plots of predicted values using base models versus actual\n",
    "t = np.arange(len(y_test))\n",
    "results = pd.DataFrame(data={'Actual': y_test, 'Lasso': y_hat_lin, 'KernelRidge': y_hat_krr,'RandomForest': y_hat_rfr, 'GradientBoosting': y_hat_gbr, 'XGBoost': y_hat_xgb, 'LightGBM': y_hat_lgb})\n",
    "results = results.sort_values(by = 'Actual', axis  = 0)\n",
    "\n",
    "colors = 'grbkmy'\n",
    "models = ['Lasso', 'KernelRidge', 'RandomForest', 'GradientBoosting', 'XGBoost', 'LightGBM']\n",
    "plt.figure(figsize=(18,12))\n",
    "for k, m in enumerate(models):\n",
    "    ax = plt.subplot(len(models)/2, 2, k+1)\n",
    "    plt.plot(t, results.Actual, label='Actual', color='magenta')\n",
    "    plt.plot(t, results[m], label=m, color=colors[k])\n",
    "    plt.xlabel('Home no.')\n",
    "    plt.ylabel('Price')\n",
    "    plt.autoscale()\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid()\n",
    "plt.suptitle(\"Predicted Ames Home versus Actual from Test Data\", fontsize=15)\n",
    "#plt.tight_layout()\n",
    "\n",
    "results.to_csv('train_test_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print mse_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#residual plots for test results\n",
    "\n",
    "models = ['Lasso', 'KernelRidge', 'RandomForest', 'GradientBoosting', 'XGBoost', 'LightGBM']\n",
    "results_residual = pd.DataFrame()\n",
    "for model in models:\n",
    "    results_residual[model] = results['Actual'] - results[model]\n",
    "results_residual['Actual'] = results['Actual']\n",
    "\n",
    "colors = 'grbkmy'\n",
    "plt.figure(figsize=(18,12))\n",
    "for i, m in enumerate(models):\n",
    "    ax = plt.subplot(len(models)/2, 2, i+1)\n",
    "    plt.plot(results_residual.Actual, results_residual[m], 'o', color=colors[i], label=m)\n",
    "    plt.xlabel('Actual Price')\n",
    "    plt.ylabel('Residual')\n",
    "    plt.xlim([10,14])\n",
    "    plt.ylim([-1,1])\n",
    "    plt.legend(loc='upper left')\n",
    "plt.suptitle(\"Residual of predicted and actual\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Base estimator prediction\n",
    "\n",
    "model = (lin, krr, rfr)\n",
    "def get_prediction(models, test_x):\n",
    "    nrow = test_x.shape[0]\n",
    "    results = np.zeros(nrow, len(models))\n",
    "    for i in range(len(models)):\n",
    "        model = models[i]\n",
    "        results[:, i] = models.predict(test_x)\n",
    "    return results    \n",
    "    \n",
    "y_pred_lin = lin.predict(d_test.drop('Id', axis=1))\n",
    "y_pred_krr = krr.predict(d_test.drop('Id', axis=1))\n",
    "y_pred_rfr = rfr.predict(d_test.drop('Id', axis=1))\n",
    "y_pred_gbr = gbr.predict(d_test.drop('Id', axis=1))\n",
    "y_pred_xgb = xgbr.predict(d_test.drop('Id', axis=1))\n",
    "y_pred_lgb = lgb.predict(d_test.drop('Id', axis=1))\n",
    "\n",
    "predictions = pd.DataFrame(data={'Id': d_test['Id'], 'Lasso': np.exp(y_pred_lin),\n",
    "                                 'KernelRidge': np.exp(y_pred_krr),'GradientBoosting': np.exp(y_pred_gbr),\n",
    "                                 'XGBoost': np.exp(y_pred_xgb), 'LightGBM': np.exp(y_pred_lgb)})\n",
    "\n",
    "predictions['mean'] = np.mean(predictions.drop('Id', axis=1), axis=1)\n",
    "predictions.to_csv('predictions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Stacking\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "import lightgbm as lgb\n",
    "\n",
    "\n",
    "#1st: Build 5 meta-models (Lasso, KRR, GBR, XGB, LGB) using train-test results, respectively.\n",
    "#2nd: Predict xs-test (base estimators' prediction data) using 5 meta-models, respectively, and then average the predicted data.\n",
    "\n",
    "results = pd.read_csv('train_test_results.csv')\n",
    "predictions = pd.read_csv('predictions.csv')\n",
    "\n",
    "xs = results[['Lasso', 'KernelRidge', 'GradientBoosting', 'XGBoost', 'LightGBM']]\n",
    "ys = results['Actual']\n",
    "\n",
    "xs_test = np.log(predictions.drop(['Id', 'mean'], axis=1))\n",
    "\n",
    "# #Lasso\n",
    "# lins = Pipeline([('Poly', PolynomialFeatures(degree=2)), ('Linear', LassoCV(alphas=np.logspace(-3,2,5), cv=3, max_iter=1e5, fit_intercept=False))])\n",
    "# lins.fit(xs, ys)\n",
    "# y_hat_lins = lins.predict(xs_test)\n",
    "# print lins\n",
    "#KernelRidge\n",
    "# krrs = KernelRidge(kernel='polynomial')\n",
    "# params_krrs = {'alpha': [0.1, 0.2, 0.5], 'degree': [2,3], 'coef0':[0.5,2.5,5]}\n",
    "# krrs = GridSearchCV(krrs, params_krrs, cv=3, n_jobs=-1, verbose=1)\n",
    "# krrs.fit(xs, ys)\n",
    "# y_hat_krrs = krrs.predict(xs_test)\n",
    "# print krrs\n",
    "#GradientBoosting\n",
    "# gbrs = GradientBoostingRegressor()\n",
    "# params_gbrs = {'learning_rate': [0.05, 0.1], 'n_estimators': [100, 500], 'max_depth': [3,5],'min_samples_split': [2,3]}\n",
    "# gbrs=GridSearchCV(gbrs, params_gbrs, cv=3, n_jobs=-1, verbose=1)\n",
    "# gbrs.fit(xs, ys)\n",
    "# y_hat_gbrs = gbrs.predict(xs_test)\n",
    "# print gbrs              \n",
    "# #XGBoost\n",
    "# xgbs = xgb.XGBRegressor()\n",
    "# params_xgbs = {'max_depth': [3,5,8], 'learning_rate': [0.05, 0.1, 0.2], 'n_estimators': [20, 100, 1000], 'objective': ['reg:linear'], 'gamma': [i/10.0 for i in range(1,3)], 'reg_alpha': [i/10.0 for i in range(0,3)], 'reg_lambda': [0, 0.2, 0.5, 0.8, 1.0]}\n",
    "# xgbs = GridSearchCV(xgbs, params_xgbs, cv=3, n_jobs=-1, verbose=1)\n",
    "# xgbs.fit(xs, ys)\n",
    "# y_hat_xgbs = xgbs.predict(xs_test)\n",
    "# print xgbs\n",
    "#LightGBM\n",
    "# lgbs = lgb.LGBMRegressor(boosting_type='gbdt', objective='regression', metric = 'mean_squared_error')\n",
    "# params_lgbs = {'learning_rate': [0.05, 0.1], 'n_estimator': [100, 500], 'max_depth': [3,5]}\n",
    "# lgbs = GridSearchCV(lgbs, params_lgbs, n_jobs=-1, verbose=1)\n",
    "# lgbs.fit(xs, ys)\n",
    "# y_hat_lgbs = lgbs.predict(xs_test)\n",
    "# print lgbs\n",
    "predictions_stack = pd.DataFrame(data={'Id': predictions['Id'], 'Lasso': np.exp(y_hat_lins), 'KernelRidge': np.exp(y_hat_krrs),\n",
    "                                      'GradientBoosting': np.exp(y_hat_gbrs), 'LightGBM': np.exp(y_hat_lgbs)})\n",
    "predictions_stack['Mean'] = np.mean(predictions_stack.drop('Id', axis=1), axis=1)\n",
    "predictions_stack.to_csv('predictions_stack.csv')\n",
    "submission = pd.DataFrame({'Id': predictions_stack['Id'], 'SalePrice': predictions_stack['Mean']})\n",
    "submission.to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
